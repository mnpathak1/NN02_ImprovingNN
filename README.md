# NN02_ImprovingNN
This is the 2nd course in the series to improve neural network performance. Follow topics are discussed:
* Regularization: penalize cost to reduce overfitting and improve testing accuracy
* Dropouts: randomly shuts down some neurons to in forward and backward prop, improves accuracy
* Gradient descent optimization method
* Momentum for faster gradient descent
* Adam optimization: combination of RMS prop and momentum
* Tensorflow exercise
